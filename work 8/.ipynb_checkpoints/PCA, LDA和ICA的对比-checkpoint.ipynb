{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习 -- PCA, LDA和ICA\n",
    "PCA, LDA 和ICA算法，常用于数据的降维分析，用于数据输入到其他模型前一些预处理，三者有相似之处，也各有不同。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 统计学基础\n",
    "给定一个m维随机变量$X=(x_1, x_2, ..., x_m)^T$，有n个观测样本构成了数据集$D$，记为$D = [X_1 \\ X_2 \\  ...\\ X_n ]$,观测样本$X_j = (x_{1j},x_{2j},...,x_{nj})^T$, 则观测数据矩阵可用以下矩阵表示：\n",
    "\n",
    "$$D =  [X_1 \\ X_2 \\  ...\\ X_n ]=\\left [\n",
    "\\begin{matrix}\n",
    "x_{11}&x_{12}&...&x_{1n}\\\\\n",
    "x_{21}&x_{22}&...&x_{2n}\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    "x_{m1}&x_{m2}&...&x_{mn}\\\\\n",
    "\\end{matrix}\n",
    "\\right]\\tag{1.1}$$\n",
    "\n",
    "有以下定义：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 散度矩阵\n",
    "散度矩阵用于描述数据的离散程度，有以下定义\n",
    "$$S =\\sum_{j=1}^n(X_j-\\bar{X})(X_j-\\bar{X})^T\\tag{1.2}$$\n",
    "假设随机变量$X$共有M个类，记为$X = \\{\\Omega_1,\\Omega_2,...,\\Omega_M\\}$，共有n个样本记为$D = [X_1, X_2,..., X_n]$,其中每类的样本数为$n_1,n_2,...,n_M$，\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 类内散度矩阵\n",
    "\n",
    "$\\Omega_i$类内的散度矩阵定义为：$$S_{w}^{(i)} = \\sum_{k=1}^{n_i}(X_k^{(i)}-\\bar{X}^{(i)})(X_k^{(i)}-\\bar{X}^{(i)})^T\\tag{1.3}$$\n",
    "总的类内散度矩阵为：$$S_w = \\sum_{i=1}^{M}S_w^{(i)}\\tag{1.4}$$\n",
    "从公式中我们可以观察到，类内散度描述的是某一类样本点与该类样本中心点的距离，总的类内散度矩阵刻画整体类内分布情况。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 类间散度矩阵\n",
    "\n",
    "$\\Omega_i$和$\\Omega_j$之间的散度矩阵（类内散度矩阵）定义为：$$S_B^{ij}=(\\bar{X}^{(i)}-\\bar{X}^{(j)})(\\bar{X}^{(i)}-\\bar{X}^{(j)})^T\\tag{1.5}$$\n",
    "总的类间散度矩阵定义为：$$S_B = \\sum_{i=1}^{M}n_i(\\bar{X}^{(i)}-\\bar{X})(\\bar{X}^{(i)}-\\bar{X})^T\\tag{1.6}$$\n",
    "类间散度矩阵刻画的是两两类样本中心点的距离，两中心点靠得越近，则两类的距离越小，总的类间散度矩阵是整体样本的一个刻画。\n",
    "\n",
    "* 总体散度矩阵\n",
    "\n",
    "总体散度矩阵为：$$S_T = S_B+S_w = \\sum_{X_i \\in D}(X_i-\\bar{X})(X_i-\\bar{X})^T\\tag{1.7}$$\n",
    "\n",
    "\n",
    "以上定义中，\n",
    "\n",
    "\n",
    "第$i$类样本的均值为：$$\\bar{X}^{(i)}=\\frac{\\sum_{j=1}^{n_i}X_{j}^{(i)}}{n_i}\\tag{1.8}$$\n",
    "\n",
    "总体样本均值为：$$\\bar{X} =\\frac{1}{n}\\sum_{X_i \\in D}X_i\\tag{1.9}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2 PCA, LDA和ICA算法\n",
    "### 2.1 PCA 主成分分析\n",
    "主成分分析，也叫PCA(Principal Component Analysis), 假设原始数据有m维特征，则PCA的主要思想就是将m维特征映射到k维特征上， 且一般情况$m> k$,从m维到k维就完成了维度的压缩。这种映射我们可以理解维投影.\n",
    "从m个维度中选择k个维度，我们自然而然就关注到这个k个维度要怎么选取，这也是PCA算法与接下来要提到的LDA算法的区别的地方.\n",
    "#### 2.1.1 PCA 算法的主要思想\n",
    "在PCA算法中，将m维特征映射到k维特征，坐标轴选取的标准为：\n",
    "选取的第一个坐标轴是原始数据方差最大的方向，选取的第二个坐标轴是与第一个坐标轴正交且方差最大的方向，依次类推，如下图，直线方向就是方差最大的方向，选取为第一主成分。\n",
    "\n",
    "给出PCA算法的数学描述：\n",
    "> 给定样本矩阵$X$, 样本的第一主成分$Y_1 = \\alpha_1^TX$是在$\\alpha_1^T\\alpha_1=1$的条件下，使得$\\alpha_1^TX_j(j=1,2,...,n)$的样本方差 $\\alpha_1^TS\\alpha_1$最大的$X$的线性变换，\n",
    "> 样本第二主成分$Y_2 = \\alpha_2^TX$是在$\\alpha_2^T\\alpha_2=1$和$\\alpha_2^TX_j$与$\\alpha_1^TX_j(j=1,2,...,n)$的样本协方差$\\alpha_1^TS\\alpha_2=0$条件下，使得$\\alpha_2^TX_j(j=1,2,...,n)$的样本方差$\\alpha_2^TS\\alpha_2$最大的$X$的线性变换。\n",
    "> 更一般地，样本的第$i$主成分$Y_i = \\alpha_i^TX$是在$\\alpha_i^T\\alpha_i=1$和$\\alpha_i^TX_j$与$\\alpha_k^TX_j(k<i, j=1,2,...,n)$的样本协方差$\\alpha_k^TS\\alpha_i=0$条件下，使得$\\alpha_i^TX_j(j=1,2,...,n)$的样本方差$\\alpha_i^TS\\alpha_i$最大的$X$的线性变换。\n",
    "\n",
    "从原始的m维数据到变换后的k维数据，k的大小需要根据具体应用来确定，通常取k使得累计方差贡献率达到规定的百分比以上，累计方差贡献率反映了k个主成分保留的信息比例。\n",
    "> 第k个主成分$Y_k$的方差贡献率定义为$Y_k$的方差与所有方差之和的比记作$\\eta_k$,$$\\eta_k = \\frac{\\lambda_k}{\\sum_{i=1}^{m} \\lambda_i}\\tag{2.1}$$\n",
    "> k个主成分$Y_1,Y_2,...,Y_k$的累计方差贡献率定义为$k$个方差之和与所有方差和之比$$\\sum_{i=1}^{k}\\eta_i = \\frac{\\sum_{i=1}^{k}\\lambda_i}{\\sum_{i=1}^{m}\\lambda_i}\\tag{2.2}$$\n",
    "\n",
    "![PCA](PCA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 PCA 分解的两种方法\n",
    "* 相关矩阵的特征值分解法\n",
    "给定样本矩阵$D$，利用数据的协方差矩阵或者样本相关矩阵的特征值分解进行主成分分析，具体步骤如下：\n",
    "> (1) 对观测数据进行规范化处理， 得到规范化数据矩阵，记为X。$$x_{ij}^{*} = \\frac{x_{ij}-\\bar{X}_i}{\\sqrt{s_{ii}}}, i=1,2,...,m; j =1,2,...,n \\tag{2.3}$$\n",
    "> 其中，$\\bar{X}_i$是变量$X$第i维数据的均值$$\\bar{x}_i = \\frac{1}{n}\\sum_{j=1}^{n}x_{ij}, i=1,2,...,m\\tag{2.4}$$\n",
    "> $s_{ii}$是第i维数据的方差，$$s_{ii} = \\frac{1}{n-1}\\sum_{j=1}^{n}(x_{ij}-\\bar{X}_i)^2, i=1,2,...,m\\tag{2.5}$$\n",
    ">\n",
    "> (2)对X计算相关矩阵R\n",
    "$$R = [r_{ij}]_{m\\times m}=\\frac{XX^T}{n-1}\\tag{2.6}$$\n",
    "其中：\n",
    "$$r_{ij}=\\frac{\\sum_{l=1}^{n}X_{il}X_{lj}}{n-1} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  i,j = 1,2,...,m\\tag{2.7}$$\n",
    ">\n",
    ">(3)求样本相关矩阵R的k个特征值和对应的k个单位特征向量。\n",
    " $$|R - \\lambda I|=0\\tag{2.8}$$\n",
    " 得到R的m个特征值\n",
    " $$\\lambda_1 \\geq \\lambda_2 \\geq ...\\geq \\lambda_m\\tag{2.9}$$\n",
    " 求使方差贡献率达到预定值得主成分个数k, 方差贡献率计算公式如下：\n",
    " $$\\sum_{i=1}^{k} \\eta_i \\geq t\\tag{2.10}$$\n",
    " 求前k个特征值对应得单位特征向量\n",
    " $$\\alpha_i = (\\alpha_{1i},\\alpha_{2i},...,\\alpha_{mi})^T, i=1,2,...,k\\tag{2.11}$$\n",
    ">\n",
    ">(4)求k个样本主成分，即以k个单位特征向量作为系数进行线性变换：\n",
    "$$Y_i = \\alpha_i^{T}X, \\ \\ \\ \\ \\ i=1,2,...,k\\tag{2.12}$$\n",
    ">\n",
    ">(5) 计算k个主成分$Y_j$与原变量$X_i$的相关系数$\\rho(X_i, Y_j)$，以及k个主成分对原变量$X_i$的贡献率$v_i$\n",
    ">\n",
    ">(6)计算n个样本的k个主成分值，将规范化样本数据代入到k个主成分式，得到n个样本的主成分值，$X_j =(x_{1j},x_{2j},...,x_{mj})^T$的第$i$主成分值是\n",
    ">$$Y_{ij} = (\\alpha_{1i},\\alpha_{2i},...,\\alpha_{mj})(x_{1j},x_{2j},...,x_{mj})^T = \\sum_{l=1}^{m}\\alpha_{li}x_{lj},i=1,2,...,m;j=1,2,...,n\\tag{2.13}$$\n",
    "\n",
    "\n",
    "* 数据矩阵的奇异值分解法\n",
    ">输入：$m \\times n$的样本矩阵$X$, 其中每一行元素的均值为0；\n",
    ">\n",
    ">输出：$k \\times n$的样本主成分矩阵$Y$\n",
    ">\n",
    ">参数：主成分个数$k$\n",
    ">\n",
    ">(1) 构造新的$n \\times m$矩阵$$X^{'} = \\frac{1}{\\sqrt{n-1}}X^T\\tag{2.16}$$\n",
    ">$X^{'}$的每一列均值为0.\n",
    ">\n",
    ">(2) 对矩阵$X^{'}$进行截断奇异值分解，得到$$X^{'} = U_k\\Sigma_k V_k^{T}\\tag{2.17}$$\n",
    ">得到k个奇异值、奇异向量。矩阵V的前k列构成了k个样本主成分。\n",
    ">\n",
    ">(3)求$k\\times n$样本主成分矩阵$$Y_k = V_k^{T}X\\tag{2.18}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 LDA 线性判别分析\n",
    "与PCA相同LDA也是一种数据降维方法，两者都是将数据投影到新的相互正交的坐标轴上，但两者在投影过程中所使用的约束条件有所不同，上面我们提到PCA是将数据投影到方差最大且相互正交的坐标轴上，尽可能多的保留原有数据的信息。在类似上述那张图的数据分布中使用PCA算法是非常合适的，但是如果遇到以下情况(如图所示)，当我们把数据投影到方差最大方向的时候，将原本分离的数据给混合在了一起，增加了分析的困难。这时候PCA算法就不再适用，因此就引入了LDA算法。\n",
    "![image.png](LDA.png)\n",
    "通过观察我们可以发现，将数据投影到黄色直线上，既可以将数据分开，还可以压缩数据，而寻找黄线的过程便是LDA压缩数据的过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 LDA算法的主要思想\n",
    "线性判别分析是一种监督学习方法，它将带标签的数据点通过投影的方法，投影到低维度中，使得样本点在低维度上可以更容易地被区分。这里的投影我们通常是指将向量投影到直线上， 一般地，对于样本矩阵$D \\subset R^m$中的任意元素$X_i$,我们希望通过一个映射关系将其变换为$R^k$空间中的向量：$$y = (y_1,y_2,...,y_k)^T\\tag{2.19}$$\n",
    "其中$k<m$,因此这个映射可以完成数据降维的目的。已知X为一个n维列向量，给定一个n维列向量$\\omega = (\\omega_1, \\omega_2,..., \\omega_m)^T$，通过\n",
    "$$y = \\omega^TX\\tag{2.20}$$\n",
    "就可以将X转化为一个数，若给定k个$\\omega$,$$W_i = (\\omega_{i1},\\omega_{i2},...,\\omega_{im})^T,i=1,2,...,k\\tag{2.21}$$\n",
    "引入矩阵\n",
    "$$W = (W_1,W_2,...,W_k)\\tag{2.22}$$\n",
    "通过映射关系\n",
    "$$Y_i = W^TX_i\\tag{2.23}$$\n",
    "可以将n维向量X，映射成为k维向量\n",
    "$$Y_i = (y_{i1},y_{i2},...,y_{ik})^T, i=1,2,...,m\\tag{2.24}$$\n",
    "其中$X_i$表示原始数据的任意一个样本值，$Y_i$表示映射后的样本值，由k个列向量组成的$W$矩阵又称为投影矩阵。LDA降维过程就是要求解出使投影后类内距离尽量小(属于同一类的样本尽可能地靠近)，类间距离尽量大（不同类样本尽量远）的投影矩阵$W$，达到该优化目标，就需要利用在第一部分提到的类间散度$S_B = \\sum_{i=1}^{M}n_i(\\bar{X}^{(i)}-\\bar{X})(\\bar{X}^{(i)}-\\bar{X})^T$和类内散度$S_w = \\sum_{i=1}^{M}S_w^{(i)}$。\n",
    "\n",
    "上述$S_B$和$S_w$都是在原始$R^m$空间上定义的，这里我们直接给出将样本点投影到$R^k$维空间后的两个散度矩阵的结论\n",
    "$$\\hat{S_B} = W^TS_BW\\tag{2.25}$$\n",
    "$$\\hat{S_w} = W^TS_wW\\tag{2.26}$$\n",
    "得到投影后的散度矩阵，我们定义优化目标函数$J$,\n",
    "$$J(W)= \\frac{det(\\hat{S_B})}{det(\\hat{S_w})}= \\frac{ W^TS_BW}{W^TS_wW}\\tag{2.27}$$\n",
    "我们知道，$\\hat{S_B}$描述样本类间距离，$\\hat{S_w}$描述样本类内距离，极大化目标函数$J(W)$我们只要极大化$det(\\hat{S_B})$,极小化$det(\\hat{S_w})$，这个优化与我们上面提到的优化目标是一样的。\n",
    "\n",
    "不失一般性，令$W^TS_wW=1$，则上式可以等价于：\n",
    "$$min_W\\ \\ \\ \\ W^TS_BW\\tag{2.28}$$\n",
    "$$st. \\ \\ \\ \\ \\ W^TS_wW\\ \\ =\\ 1\\tag{2.29}$$\n",
    "我们可以通过拉格朗日乘子法来优化上述目标函数,上式等价于：\n",
    "$$C(W) = W^TS_BW - \\lambda( W^TS_wW-1)\\tag{2.30}$$\n",
    "对上式求导，最终可以的得到：\n",
    "$$S_W^{-1}S_BW = \\lambda W\\tag{2.31}$$\n",
    "这就变成了一个求解矩阵特征向量的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 LDA算法流程\n",
    "使用LDA算法，对数据进行降维，具体步骤如下：\n",
    ">输入: 数据集$D \\in R^m$,$D = \\{(X_i,L_i)\\}$,其中$i=1,2,...,n$表示一共有n个样本，$L_i \\in\\{1,2,...,M\\}$共有M类样本，L表示标签\n",
    ">\n",
    ">输出：降维后的数据集,$D^{'}\\in R^k$\n",
    ">\n",
    ">(1)根据公式（1.3）,（1.4）计算类内散度矩阵$S_w$:$$S_w = \\sum_{i=1}^{M}S_w^{(i)}= \\sum_{i=1}^{M}\\sum_{k=1}^{n_i}(X_k^{(i)}-\\bar{X}^{(i)})(X_k^{(i)}-\\bar{X}^{(i)})^T$$\n",
    ">(2)根据公式（1.6）计算类间散度矩阵：$$S_B = \\sum_{i=1}^{M}n_i(\\bar{X}^{(i)}-\\bar{X})(\\bar{X}^{(i)}-\\bar{X})^T$$\n",
    ">(3)计算矩阵：$$S_w^{-1}S_B$$\n",
    ">(4)对$S_w^{-1}S_B$进行奇异值分解，得到奇异值$\\lambda_i$以及对应的特征向量$\\omega_i$,$i=1,2,...,n-1$\n",
    ">\n",
    ">(5)取前k大的奇异值对应的特征向量组成投影矩阵$W$\n",
    ">\n",
    ">(6)计算每个样本$X_i$在新的k维空间中的投影$Y_i$:$$Y_i = W^TX_i$$\n",
    ">(7)得到降维以后的数据集$D^{'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ICA 独立成分分析\n",
    "独立主成分分析，又被称为ICA，是一种用来从多变量统计数据中寻找到隐含的因素或者成分的方法，被应用在多种领域，被认为是PCA和FA的一种拓展。ICA算法本质上是找出构成信号相互独立的部分，为观察数据定义了一个生成模型，在这个模型中，其认为观测数据矩阵X是由独立元（相互独立的部分）经过矩阵A线性加权获得，如下：\n",
    "$$X = AS$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 ICA问题的表述\n",
    "$x$为一个m维向量，$X \\in R^m$,即\n",
    "$$X = (x_1,x_2,...,x_m)^T \\tag{2.32}$$\n",
    "$X$的m个维度是相互非独立的，在一定假设条件下，我们可以用m个独立的变量的线性组合来重新表示$X$,如下:\n",
    "$$(x_1,x_2,...,x_m)^T = A(s_1,s_2,...,s_m)^T\\tag{2.33}$$\n",
    "其中，$s_i$两两相互独立，$A$是满秩矩阵，$A \\in R^{m\\times m}$,令\n",
    "$$S = (s_1,s_2,...,s_m)^T\\tag{2.34}$$\n",
    "则：\n",
    "$$X = AS\\tag{2.35}$$\n",
    "又可以表示为：\n",
    "$$S = WX\\tag{2.36}$$\n",
    "其中，$W = A^{-1}$,$W \\in R^{m\\times m}$\n",
    "假设$X$共有n个样本点，$D = \\{X_1,X_2,...,X_n\\}$,则，我们记数据矩阵如公式(1.1)的形式，独立主成分分析的目标就是在只知道数据矩阵的情况下，估算$A,W,S$的取值，其中最经典的实例：在一个大厅里，有m个人在聊天（分别对应着X的m个维度，两两之间不独立），在大厅的不同角落布置了m个麦克风记录大厅的声音，一共记录n秒（对应了n个样本点），ICA的目标就是从混合声音中把每个人的声音给单独分离出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 ICA 问题求解\n",
    "由前我们可知，\n",
    "$$s_i = w_iX\\tag{2.37}$$\n",
    "其中，$w_i = (w_{i,1},w_{i,2},...,w_{i,m})$,每一个$w_i$都可以将x转化为一个数，即$S$的的一个维度$s_i$，设随机变量$s_i$概率密度函数是$p_{s_i}(s_i)$,同时我们假设x的概率密度函数为$p_{x}(x)$,我们是可以通过$s_i$的概率密度函数求x的概率密度函数的，这里直接给出结论：\n",
    "$$p_{X}(x) = ||W||\\prod_{i=1}^{m}p_{s_i}(w_iX)\\tag{2.38}$$\n",
    "接下来要做的就是根据数据集$D$计算$W$的值，数据集$D$出现的概率是:\n",
    "$$L = \\prod_{i=1}^{n}(||W||\\prod_{j=1}^{m}p_{s_j}(w_jX_i))\\tag{2.39}$$\n",
    "其中$X_i = (x_{1,i},x_{2,i},...,x_{m,i})^T$，对(2.39)式两边取自然对数，则：\n",
    "$$ln L = \\sum_{i=1}^{n}(ln||W||+\\sum_{j=1}^{m}(lnp_{s_j}(w_{j}X_{i})))=\\sum_{i=1}^{n}\\sum_{j=1}^{m}ln p_{s_j}(w_jX_i)+mln||W||\\tag{2.40}$$\n",
    "当$ln L$取得最大值时，L也取得最大值，使用梯度下降法对（2.40）进行优化，最终可得W的更新公式为：\n",
    "$$W = W +\\alpha(Z^TD+n(W^{-1})^T)\\tag{2.41}$$\n",
    "其中$\\alpha$为学习率，Z为：\n",
    "$$Z = g(K)=g(WD)\\tag{2.42}$$\n",
    "$$g(x) = \\frac{1-e^x}{1+e^x}\\tag{2.43}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "理论上通过上述过程就可以迭代出数据集$X$出现时的$W$,进而求解出$A$,$S$,完成ICA问题的求解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（1）相比于其他两种算法，PCA算法具有更强的通用性。与ICA相比，PCA更容易获得稳定的主成分，同时与LDA相比较，PCA不要过分在意数据本身的类别信息，因为PCA算法为无监督学习算法。\n",
    "\n",
    "（2）PCA、LDA的假设上都假设了数据的分布维高斯分布，而ICA是非高斯分布，因此在一些非高斯分布的数据集上，前两者的效果未必好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "（3）PCA与LDA在后面的几个步骤基本上是相同的，可以获得1~n维有排序关系的特征，一般情况下，排序越前的特征所包含的信息量越高。PCA旨在于去除原始数据中的冗余特征，是的投影在各个维度的方差尽可能大，而且是相互正交的。而LDA使得数据的类内距离小，类间距离大，较为关注的是样本的分类问题，其不保证投影到的新坐标系是正交的，也就是不同的数据之间可能还存在一定的相关性。与前者不同的是，ICA并不认为随机信号最有用的信息体现在类间或是最大方差里，而是构成样本集的独立成分，实际应用中ICA并不能起到降维的效果，也不单独使用，通常会和PCA或者白化处理结合使用。ICA的输出维数和输入相同，相互独立，没有排序关系，在某种意义上更具有区分度。其是一种数据预处理方式，在因果关系分析问题中应用广泛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
