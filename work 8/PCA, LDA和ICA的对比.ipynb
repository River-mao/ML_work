{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 机器学习 -- PCA, LDA和ICA\n",
    "PCA, LDA 和ICA算法，常用于数据的降维分析，用于数据输入到其他模型前一些预处理，三者有相似之处，也各有不同。\n",
    "\n",
    "## 1 统计学基础\n",
    "给定一个m维随机变量$X=(x_1, x_2, ..., x_m)^T$，有n个观测样本 $X = [X_1 \\ X_2 \\  ...\\ X_n ]$,观测样本$X_j = (x_{1j},x_{2j},...,x_{nj})^T$, 则观测数据矩阵可用以下矩阵表示：\n",
    "\n",
    "$$X =  [X_1 \\ X_2 \\  ...\\ X_n ]=\\left [\n",
    "\\begin{matrix}\n",
    "x_{11}&x_{12}&...&x_{1n}\\\\\n",
    "x_{21}&x_{22}&...&x_{2n}\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    "x_{m1}&x_{m2}&...&x_{mn}\\\\\n",
    "\\end{matrix}\n",
    "\\right]$$\n",
    "\n",
    "有以下定义：\n",
    "### 1.1 散度矩阵\n",
    "$$S =\\sum_{j=1}^n(X_j-\\bar{X})(X_j-\\bar{X})^T$$\n",
    "\n",
    "### 1.2 类间散度矩阵与类内散度矩阵\n",
    "假设随机变量$X$共有M个类，记为$X = \\{\\Omega_1,\\Omega_2,...,\\Omega_M\\}$，共有n个样本记为$X = [X_1, X_2,..., X_n]$,其中每类的样本数为$n_1,n_2,...,n_M$，\n",
    "\n",
    "$\\Omega_i$类内的散度矩阵定义为：$$S_{w}^{(i)} = \\frac{\\sum_{k=1}^{n_i}(X_k^{(i)}-\\bar{X}^{(i)})(X_k^{(i)}-\\bar{X}^{(i)})^T}{n_i}$$\n",
    "总的类内散度矩阵为：$$S_w = \\sum_{i=1}^{M}P(\\Omega_i)S_w^{(i)}$$\n",
    "$\\Omega_i$和$\\Omega_j$之间的散度矩阵（类内散度矩阵）定义为：$$S_B^{ij}=(\\bar{X}^{(i)}-\\bar{X}^{(j)})(\\bar{X}^{(i)}-\\bar{X}^{(j)})^T$$\n",
    "总的类间散度矩阵定义为：$$S_B = \\frac{1}{2}\\sum_{i=1}^MP(\\Omega_i)\\sum_{j=1}^{M}P(\\Omega_j)S_B^{ij}$$\n",
    "总体散度矩阵为：$$S_T = S_B+S_w = \\frac{1}{N}\\sum_{i=1}^M(X_i-\\bar{X})(X_i-\\bar{X})^T$$\n",
    "\n",
    "以上定义中，\n",
    "\n",
    "第$i$类样本的概率为：$$P(\\Omega_i) = \\frac{n_i}{n}$$\n",
    "\n",
    "第$i$类样本的均值为：$$\\bar{X}^{(i)}=\\frac{\\sum_{j=1}^{n_i}X_{j}^{(i)}}{n_i}$$\n",
    "\n",
    "总体样本均值为：$$\\bar{X} =\\sum_{i=1}^{M}P(\\Omega_i)\\bar{X}^{(i)}$$\n",
    "## 2 PCA, LDA和ICA算法\n",
    "### 2.1 PCA 主成分分析\n",
    "主成分分析，也叫PCA(Principal Component Analysis), 假设原始数据有m维特征，则PCA的主要思想就是将m维特征映射到k维特征上， 且一般情况$m> k$,从m维到k维就完成了维度的压缩。这种映射我们可以理解维投影.\n",
    "从m个维度中选择k个维度，我们自然而然就关注到这个k个维度要怎么选取，这也是PCA算法与接下来要提到的LDA算法的区别的地方.\n",
    "#### 2.1.1 PCA 算法的主要思想\n",
    "在PCA算法中，将m维特征映射到k维特征，坐标轴选取的标准为：\n",
    "选取的第一个坐标轴是原始数据方差最大的方向，选取的第二个坐标轴是与第一个坐标轴正交且方差最大的方向，依次类推。\n",
    "给出这个PCA算法的数学描述：\n",
    "> 给定样本矩阵$X$, 样本的第一主成分$Y_1 = \\alpha_1^TX$是在$\\alpha_1^T\\alpha_1=1$的条件下，使得$\\alpha_1^TX_j(j=1,2,...,n)$的样本方差 $\\alpha_1^TS\\alpha_1$最大的$X$的线性变换，\n",
    "> 样本第二主成分$Y_2 = \\alpha_2^TX$是在$\\alpha_2^T\\alpha_2=1$和$\\alpha_2^TX_j$与$\\alpha_1^TX_j(j=1,2,...,n)$的样本协方差$\\alpha_1^TS\\alpha_2=0$条件下，使得$\\alpha_2^TX_j(j=1,2,...,n)$的样本方差$\\alpha_2^TS\\alpha_2$最大的$X$的线性变换。\n",
    "> 更一般地，样本的第$i$主成分$Y_i = \\alpha_i^TX$是在$\\alpha_i^T\\alpha_i=1$和$\\alpha_i^TX_j$与$\\alpha_k^TX_j(k<i, j=1,2,...,n)$的样本协方差$\\alpha_k^TS\\alpha_i=0$条件下，使得$\\alpha_i^TX_j(j=1,2,...,n)$的样本方差$\\alpha_i^TS\\alpha_i$最大的$X$的线性变换。\n",
    "\n",
    "从原始的m维数据到变换后的k维数据，k的大小需要根据具体应用来确定，通常取k使得累计方差贡献率达到规定的百分比以上，累计方差贡献率反映了k个主成分保留的信息比例。\n",
    "> 第k个主成分$Y_k$的方差贡献率定义为$Y_k$的方差与所有方差之和的比记作$\\eta_k$,$$\\eta_k = \\frac{\\lambda_k}{\\sum_{i=1}^{m} \\lambda_i}$$\n",
    "> k个主成分$Y_1,Y_2,...,Y_k$的累计方差贡献率定义为$k$个方差之和与所有方差和之比$$\\sum_{i=1}^{k}\\eta_i = \\frac{\\sum_{i=1}^{k}\\lambda_i}{\\sum_{i=1}^{m}\\lambda_i}$$\n",
    "\n",
    "#### 2.1.2 PCA 分解的两种方法\n",
    "* 相关矩阵的特征值分解法\n",
    "* 数据矩阵的奇异值分解法\n",
    "### 2.2 LDA 线性判别分析\n",
    "### 2.3 ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}