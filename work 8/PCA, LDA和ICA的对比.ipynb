{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 机器学习 -- PCA, LDA和ICA\n",
    "PCA, LDA 和ICA算法，常用于数据的降维分析，用于数据输入到其他模型前一些预处理，三者有相似之处，也各有不同。\n",
    "\n",
    "## 1 统计学基础\n",
    "给定一个m维随机变量$X=(x_1, x_2, ..., x_m)^T$，有n个观测样本 $X = [X_1 \\ X_2 \\  ...\\ X_n ]$,观测样本$X_j = (x_{1j},x_{2j},...,x_{nj})^T$, 则观测数据矩阵可用以下矩阵表示：\n",
    "\n",
    "$$X =  [X_1 \\ X_2 \\  ...\\ X_n ]=\\left [\n",
    "\\begin{matrix}\n",
    "x_{11}&x_{12}&...&x_{1n}\\\\\n",
    "x_{21}&x_{22}&...&x_{2n}\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    ".&.& &.\\\\\n",
    "x_{m1}&x_{m2}&...&x_{mn}\\\\\n",
    "\\end{matrix}\n",
    "\\right]$$\n",
    "\n",
    "有以下定义：\n",
    "### 1.1 散度矩阵\n",
    "$$S =\\sum_{j=1}^n(X_j-\\bar{X})(X_j-\\bar{X})^T$$\n",
    "\n",
    "### 1.2 类间散度矩阵与类内散度矩阵\n",
    "假设随机变量$X$共有M个类，记为$X = \\{\\Omega_1,\\Omega_2,...,\\Omega_M\\}$，共有n个样本记为$X = [X_1, X_2,..., X_n]$,其中每类的样本数为$n_1,n_2,...,n_M$，\n",
    "\n",
    "$\\Omega_i$类内的散度矩阵定义为：$$S_{w}^{(i)} = \\frac{\\sum_{k=1}^{n_i}(X_k^{(i)}-\\bar{X}^{(i)})(X_k^{(i)}-\\bar{X}^{(i)})^T}{n_i}$$\n",
    "总的类内散度矩阵为：$$S_w = \\sum_{i=1}^{M}P(\\Omega_i)S_w^{(i)}$$\n",
    "$\\Omega_i$和$\\Omega_j$之间的散度矩阵（类内散度矩阵）定义为：$$S_B^{ij}=(\\bar{X}^{(i)}-\\bar{X}^{(j)})(\\bar{X}^{(i)}-\\bar{X}^{(j)})^T$$\n",
    "总的类间散度矩阵定义为：$$S_B = \\frac{1}{2}\\sum_{i=1}^MP(\\Omega_i)\\sum_{j=1}^{M}P(\\Omega_j)S_B^{ij}$$\n",
    "总体散度矩阵为：$$S_T = S_B+S_w = \\frac{1}{N}\\sum_{i=1}^M(X_i-\\bar{X})(X_i-\\bar{X})^T$$\n",
    "\n",
    "以上定义中，\n",
    "\n",
    "第$i$类样本的概率为：$$P(\\Omega_i) = \\frac{n_i}{n}$$\n",
    "\n",
    "第$i$类样本的均值为：$$\\bar{X}^{(i)}=\\frac{\\sum_{j=1}^{n_i}X_{j}^{(i)}}{n_i}$$\n",
    "\n",
    "总体样本均值为：$$\\bar{X} =\\sum_{i=1}^{M}P(\\Omega_i)\\bar{X}^{(i)}$$\n",
    "## 2 PCA, LDA和ICA算法\n",
    "### 2.1 PCA 主成分分析\n",
    "主成分分析，也叫PCA(Principal Component Analysis), 假设原始数据有m维特征，则PCA的主要思想就是将m维特征映射到k维特征上， 且一般情况$m> k$,从m维到k维就完成了维度的压缩。这种映射我们可以理解维投影.\n",
    "从m个维度中选择k个维度，我们自然而然就关注到这个k个维度要怎么选取，这也是PCA算法与接下来要提到的LDA算法的区别的地方.\n",
    "#### 2.1.1 PCA 算法的主要思想\n",
    "在PCA算法中，将m维特征映射到k维特征，坐标轴选取的标准为：\n",
    "选取的第一个坐标轴是原始数据方差最大的方向，选取的第二个坐标轴是与第一个坐标轴正交且方差最大的方向，依次类推。\n",
    "给出这个PCA算法的数学描述：\n",
    "> 给定样本矩阵$X$, 样本的第一主成分$Y_1 = \\alpha_1^TX$是在$\\alpha_1^T\\alpha_1=1$的条件下，使得$\\alpha_1^TX_j(j=1,2,...,n)$的样本方差 $\\alpha_1^TS\\alpha_1$最大的$X$的线性变换，\n",
    "> 样本第二主成分$Y_2 = \\alpha_2^TX$是在$\\alpha_2^T\\alpha_2=1$和$\\alpha_2^TX_j$与$\\alpha_1^TX_j(j=1,2,...,n)$的样本协方差$\\alpha_1^TS\\alpha_2=0$条件下，使得$\\alpha_2^TX_j(j=1,2,...,n)$的样本方差$\\alpha_2^TS\\alpha_2$最大的$X$的线性变换。\n",
    "> 更一般地，样本的第$i$主成分$Y_i = \\alpha_i^TX$是在$\\alpha_i^T\\alpha_i=1$和$\\alpha_i^TX_j$与$\\alpha_k^TX_j(k<i, j=1,2,...,n)$的样本协方差$\\alpha_k^TS\\alpha_i=0$条件下，使得$\\alpha_i^TX_j(j=1,2,...,n)$的样本方差$\\alpha_i^TS\\alpha_i$最大的$X$的线性变换。\n",
    "\n",
    "从原始的m维数据到变换后的k维数据，k的大小需要根据具体应用来确定，通常取k使得累计方差贡献率达到规定的百分比以上，累计方差贡献率反映了k个主成分保留的信息比例。\n",
    "> 第k个主成分$Y_k$的方差贡献率定义为$Y_k$的方差与所有方差之和的比记作$\\eta_k$,$$\\eta_k = \\frac{\\lambda_k}{\\sum_{i=1}^{m} \\lambda_i}$$\n",
    "> k个主成分$Y_1,Y_2,...,Y_k$的累计方差贡献率定义为$k$个方差之和与所有方差和之比$$\\sum_{i=1}^{k}\\eta_i = \\frac{\\sum_{i=1}^{k}\\lambda_i}{\\sum_{i=1}^{m}\\lambda_i}$$\n",
    "\n",
    "#### 2.1.2 PCA 分解的两种方法\n",
    "* 相关矩阵的特征值分解法\n",
    "给定样本矩阵X，利用数据的协方差矩阵或者样本相关矩阵的特征值分解进行主成分分析，具体步骤如下：\n",
    "> (1) 对观测数据进行规范化处理， 得到规范化数据矩阵，仍记为X。$$x_{ij}^{*} = \\frac{x_{ij}-\\bar{X}_i}{\\sqrt{s_{ii}}}, i=1,2,...,m; j =1,2,...,n$$\n",
    "> 其中，$\\bar{X}_i$是变量$X$第i维数据的均值$$\\bar{x}_i = \\frac{1}{n}\\sum_{j=1}^{n}x_{ij}, i=1,2,...,m$$\n",
    "> $s_{ii}$是第i维数据的方差，$$s_{ii} = \\frac{1}{n-1}\\sum_{j=1}^{n}(x_{ij}-\\bar{X}_i)^2, i=1,2,...,m$$\n",
    ">\n",
    "> (2)对X计算相关矩阵R\n",
    "$$R = [r_{ij}]_{m\\times m}=\\frac{XX^T}{n-1}$$\n",
    "其中：\n",
    "$$r_{ij}=\\frac{\\sum_{l=1}^{n}X_{il}X_{lj}}{n-1} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\  i,j = 1,2,...,m$$\n",
    ">\n",
    ">(3)求样本相关矩阵R的k个特征值和对应的k个单位特征向量。\n",
    " $$|R - \\lambda I|=0$$\n",
    " 得到R的m个特征值\n",
    " $$\\lambda_1 \\geq \\lambda_2 \\geq ...\\geq \\lambda_m$$\n",
    " 求使方差贡献率达到预定值得主成分个数k, 方差贡献率计算公式如下：\n",
    " $$\\sum_{i=1}^{k} \\eta_i \\geq t$$\n",
    " 求前k个特征值对应得单位特征向量\n",
    " $$\\alpha_i = (\\alpha_{1i},\\alpha_{2i},...,\\alpha_{mk})^T$$\n",
    ">\n",
    ">(4)求k个样本主成分，即以k个单位特征向量作为系数进行线性变换：\n",
    "$$Y_i = \\alpha_i^{T}X, \\ \\ \\ \\ \\ i=1,2,...,k$$\n",
    ">\n",
    ">(5) 计算k个主成分$Y_j$与原变量$X_i$的相关系数$\\rho(X_i, Y_j)$，以及k个主成分对原变量$X_i$的贡献率$v_i$\n",
    ">\n",
    ">(6)计算n个样本的k个主成分值，将规范化样本数据代入到k个主成分式，得到n个样本的主成分值，$X_j =(x_{1j},x_{2j},...,x_{mj})^T$的第$i$主成分值是\n",
    ">$$Y_{ij} = (\\alpha_{1i},\\alpha_{2i},...,\\alpha_{mj})(x_{1j},x_{2j},...,x_{mj})^T = \\sum_{l=1}^{m}\\alpha_{li}x_{lj},i=1,2,...,m;j=1,2,...,n$$\n",
    ">\n",
    "* 数据矩阵的奇异值分解法\n",
    "\n",
    "### 2.2 LDA 线性判别分析\n",
    "\n",
    "### 2.3 ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}