{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业三：改写决策树\n",
    "\n",
    "### 班级：人工智能与机器人班\n",
    "\n",
    "### 学号：201700171080\n",
    "\n",
    "### 姓名：毛冬辉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 作业说明\n",
    "C45与ID3决策树的区别仅在于，前者的启发式函数为信息增益比，后者为信息增益。\n",
    "\n",
    "信息增益：$$g(D,A) = H(D)-H(D|A)$$\n",
    "信息增益比：$$g_r(D,A) = \\frac{g(D,A)}{H_A(D)}$$\n",
    "\n",
    "信息增益与信息增益比只相差一个，训练数据集D关于特征A的值的熵 $H_A(D)$.\n",
    "而原代码使用ID3实现，只需额外求解一个 $H_A(D)$.\n",
    "\n",
    "见代码中$\\bf{calc\\_ent\\_had(A)}$函数与$\\bf{recurse\\_train函数中的步骤3}.$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:start binaryzation_features()\n",
      "DEBUG:root:end binaryzation_features(), cost 0.6123631000518799 seconds\n",
      "DEBUG:root:start train()\n",
      "DEBUG:root:end train(), cost 128.132714509964 seconds\n",
      "DEBUG:root:start predict()\n",
      "DEBUG:root:end predict(), cost 0.07779097557067871 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accruacy socre is 0.9575036075036075\n"
     ]
    }
   ],
   "source": [
    "# encoding=utf-8\n",
    "import cv2\n",
    "import time\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "total_class = 10\n",
    "\n",
    "\n",
    "def log(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        logging.debug('start %s()' % func.__name__)\n",
    "        ret = func(*args, **kwargs)\n",
    "\n",
    "        end_time = time.time()\n",
    "        logging.debug('end %s(), cost %s seconds' % (func.__name__, end_time - start_time))\n",
    "\n",
    "        return ret\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "# 二值化\n",
    "def binaryzation(img):\n",
    "    cv_img = img.astype(np.uint8)\n",
    "    cv2.threshold(cv_img, 50, 1, cv2.THRESH_BINARY_INV, cv_img)\n",
    "    return cv_img\n",
    "\n",
    "\n",
    "@log\n",
    "def binaryzation_features(trainset):\n",
    "    features = []\n",
    "\n",
    "    for img in trainset:\n",
    "        img = np.reshape(img, (28, 28))\n",
    "        cv_img = img.astype(np.uint8)\n",
    "\n",
    "        img_b = binaryzation(cv_img)\n",
    "        # hog_feature = np.transpose(hog_feature)\n",
    "        features.append(img_b)\n",
    "\n",
    "    features = np.array(features)\n",
    "    features = np.reshape(features, (-1, 784))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "class Tree(object):\n",
    "    def __init__(self, node_type, Class=None, feature=None):\n",
    "        self.node_type = node_type\n",
    "        self.dict = {}\n",
    "        self.Class = Class\n",
    "        self.feature = feature\n",
    "\n",
    "    def add_tree(self, val, tree):\n",
    "        self.dict[val] = tree\n",
    "\n",
    "    def predict(self, features):\n",
    "        if self.node_type == 'leaf':\n",
    "            return self.Class\n",
    "\n",
    "        tree = self.dict[features[self.feature]]\n",
    "        return tree.predict(features)\n",
    "\n",
    "\n",
    "def calc_ent(x):\n",
    "    \"\"\"\n",
    "        calculate shanno ent of x\n",
    "        计算经验熵\n",
    "    \"\"\"\n",
    "\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        # x[x == x_value].shape[0] 计算x中元素值为x_value的个数\n",
    "        p = float(x[x == x_value].shape[0]) / x.shape[0]\n",
    "        logp = np.log2(p)\n",
    "        ent -= p * logp\n",
    "\n",
    "    return ent\n",
    "\n",
    "def calc_condition_ent(x, y):\n",
    "    \"\"\"\n",
    "        calculate ent H(y|x)\n",
    "        计算条件经验熵\n",
    "    \"\"\"\n",
    "    # calc ent(y|x)\n",
    "    x_value_list = set([x[i] for i in range(x.shape[0])])\n",
    "    ent = 0.0\n",
    "    for x_value in x_value_list:\n",
    "        sub_y = y[x == x_value]\n",
    "        temp_ent = calc_ent(sub_y)\n",
    "        ent += (float(sub_y.shape[0]) / y.shape[0]) * temp_ent\n",
    "\n",
    "    return ent\n",
    "\n",
    "\n",
    "def calc_ent_grap(x, y):\n",
    "    \"\"\"\n",
    "        calculate ent grap\n",
    "    \"\"\"\n",
    "    base_ent = calc_ent(y)\n",
    "    condition_ent = calc_condition_ent(x, y)\n",
    "    ent_grap = base_ent - condition_ent\n",
    "\n",
    "    return ent_grap\n",
    "#-------------------------------------------------------------------------------\n",
    "#---------------------将ID3改为C45的关键部分--------------------------------------\n",
    "#---------------------计算数据集D关于特征A的熵------------------------------------\n",
    "def calc_ent_had(A):\n",
    "    '''\n",
    "    calculate entropy for D with feature:A\n",
    "    '''\n",
    "    A = np.array(A)\n",
    "    feature_value_list = set([A[i] for i in range(A.shape[0])])\n",
    "\n",
    "    # 由于 ent_hda 在计算信息增益比的时候在分母位置，将其赋值为一个非0值，避免出发错误\n",
    "    ent_hda = 0.0000001\n",
    "    for feature_value in feature_value_list:\n",
    "        p = float(A[A == feature_value].shape[0])/float(A.shape[0])\n",
    "        logp = np.log2(p)\n",
    "        ent_hda -= p * logp\n",
    "\n",
    "    return ent_hda\n",
    "\n",
    "def recurse_train(train_set, train_label, features, epsilon):\n",
    "    global total_class\n",
    "\n",
    "    LEAF = 'leaf'\n",
    "    INTERNAL = 'internal'\n",
    "\n",
    "    # 步骤1——如果train_set中的所有实例都属于同一类Ck\n",
    "    label_set = set(train_label)  # set返回list中无序不重复的数\n",
    "    if len(label_set) == 1:\n",
    "        return Tree(LEAF, Class=label_set.pop())\n",
    "\n",
    "    # 步骤2——如果features为空\n",
    "    (max_class, max_len) = max([(i, len(list(filter(lambda x: x == i, train_label)))) for i in range(total_class)],\n",
    "                               key=lambda x: x[1])\n",
    "\n",
    "    if len(list(features)) == 0:\n",
    "        return Tree(LEAF, Class=max_class)\n",
    "\n",
    "    # 步骤3——计算信息增益比\n",
    "    max_feature = 0\n",
    "    max_grda = 0\n",
    "\n",
    "    D = train_label\n",
    "    HD = calc_ent(D)\n",
    "\n",
    "    for feature in features:\n",
    "        A = np.array(train_set[:, feature].flat)\n",
    "        gda = HD - calc_condition_ent(A, D)\n",
    "\n",
    "        #----------------------------------------------------------\n",
    "        #----------------计算训练数据集D关于特征A的值的熵HA(D)--------\n",
    "        ent_hda = calc_ent_had(A)\n",
    "        # ---------------计算信息增益比-----------------------------\n",
    "        grda = gda/ent_hda\n",
    "\n",
    "        if grda > max_grda:\n",
    "            max_grda, max_feature = grda, feature\n",
    "\n",
    "    # 步骤4——小于阈值\n",
    "    if max_grda < epsilon:\n",
    "        return Tree(LEAF, Class=max_class)\n",
    "\n",
    "    # 步骤5——构建非空子集\n",
    "    sub_features = list(filter(lambda x: x != max_feature, features))  # 去掉当前特征\n",
    "    tree = Tree(INTERNAL, feature=max_feature)\n",
    "\n",
    "    feature_col = np.array(train_set[:, max_feature].flat)\n",
    "    feature_value_list = set([feature_col[i] for i in range(feature_col.shape[0])])\n",
    "    for feature_value in feature_value_list:\n",
    "\n",
    "        index = []\n",
    "        for i in range(len(train_label)):\n",
    "            if train_set[i][max_feature] == feature_value:\n",
    "                index.append(i)\n",
    "\n",
    "        sub_train_set = train_set[index]\n",
    "        sub_train_label = train_label[index]\n",
    "\n",
    "        sub_tree = recurse_train(sub_train_set, sub_train_label, sub_features, epsilon)\n",
    "        tree.add_tree(feature_value, sub_tree)\n",
    "\n",
    "    return tree\n",
    "\n",
    "\n",
    "@log\n",
    "def train(train_set, train_label, features, epsilon):\n",
    "    return recurse_train(train_set, train_label, features, epsilon)\n",
    "\n",
    "\n",
    "@log\n",
    "def predict(test_set, tree):\n",
    "    result = []\n",
    "    for features in test_set:\n",
    "        tmp_predict = tree.predict(features)\n",
    "        result.append(tmp_predict)\n",
    "    return np.array(result)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    logger = logging.getLogger()\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # https://github.com/WenDesi/lihang_book_algorithm/blob/master/data/test.csv\n",
    "    raw_data = pd.read_csv('./train_binary.csv', header=0)\n",
    "    data = raw_data.values\n",
    "\n",
    "    imgs = data[0::, 1::]\n",
    "    labels = data[::, 0]\n",
    "\n",
    "    # 图片二值化\n",
    "    features = binaryzation_features(imgs)\n",
    "\n",
    "    # 选取 2/3 数据作为训练集， 1/3 数据作为测试集\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33,\n",
    "                                                                                random_state=23323)\n",
    "\n",
    "    tree = train(train_features, train_labels, [i for i in range(784)], 0.1)\n",
    "    test_predict = predict(test_features, tree)\n",
    "    score = accuracy_score(test_labels, test_predict)\n",
    "\n",
    "print(\"The accruacy socre is {}\".format(score))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('cook': conda)",
   "language": "python",
   "name": "python361064bitcookconda2d3f5abb2ed14be8a6562d66d7e8fe13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
