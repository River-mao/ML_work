{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 机器学习-作业5\n",
    "\n",
    "## 1 boosting算法实现\n",
    "使用boosting算法实现iris数据集的分类，决策树桩 <v和>v"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.1 代码实现："
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "---------------------------------\n",
      "-----------初始化----------------\n",
      "---------------------------------\n",
      "数据量:60\n",
      "训练数据量:45\n",
      "测试数据量:15\n",
      "D=[0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222 0.02222222\n",
      " 0.02222222 0.02222222 0.02222222]\n",
      "-------------------------------------\n",
      "-------------第1个决策树-------------\n",
      "-------------------------------------\n",
      "决策树桩：v = 2.775\n",
      "权重：alpha = 0.7657039801551192\n",
      "权值向量：D=[0.01351398 0.01351398 0.01351398 0.01351398 0.01351398 0.01351398\n",
      " 0.01351398 0.06249786 0.01351398 0.01351398 0.06249786 0.01351398\n",
      " 0.01351398 0.06249786 0.01351398 0.01351398 0.01351398 0.06249786\n",
      " 0.01351398 0.01351398 0.06249786 0.01351398 0.01351398 0.01351398\n",
      " 0.01351398 0.06249786 0.01351398 0.06249786 0.01351398 0.01351398\n",
      " 0.01351398 0.01351398 0.01351398 0.01351398 0.01351398 0.01351398\n",
      " 0.01351398 0.01351398 0.01351398 0.01351398 0.01351398 0.06249786\n",
      " 0.01351398 0.01351398 0.01351398]\n",
      "-------------------------------------\n",
      "-------------第2个决策树-------------\n",
      "-------------------------------------\n",
      "决策树桩：v = 3.25\n",
      "权重：alpha = 0.48806724689755376\n",
      "权值向量：D=[0.00930289 0.00930289 0.02469151 0.00930289 0.02469151 0.00930289\n",
      " 0.00930289 0.04302291 0.00930289 0.02469151 0.04302291 0.00930289\n",
      " 0.00930289 0.04302291 0.02469151 0.00930289 0.02469151 0.11419041\n",
      " 0.00930289 0.00930289 0.11419041 0.00930289 0.00930289 0.00930289\n",
      " 0.00930289 0.04302291 0.02469151 0.04302291 0.00930289 0.00930289\n",
      " 0.02469151 0.00930289 0.00930289 0.02469151 0.00930289 0.00930289\n",
      " 0.00930289 0.00930289 0.00930289 0.00930289 0.02469151 0.04302291\n",
      " 0.02469151 0.00930289 0.02469151]\n",
      "-------------------------------------\n",
      "-------------第3个决策树-------------\n",
      "-------------------------------------\n",
      "决策树桩：v = 2.3\n",
      "权重：alpha = 0.22167362387180325\n",
      "权值向量：D=[0.00763717 0.00763717 0.02027039 0.00763717 0.02027039 0.00763717\n",
      " 0.00763717 0.05502469 0.00763717 0.02027039 0.05502469 0.00763717\n",
      " 0.00763717 0.05502469 0.02027039 0.00763717 0.02027039 0.09374415\n",
      " 0.00763717 0.00763717 0.14604525 0.00763717 0.00763717 0.00763717\n",
      " 0.00763717 0.05502469 0.02027039 0.05502469 0.00763717 0.00763717\n",
      " 0.02027039 0.00763717 0.01189804 0.02027039 0.01189804 0.00763717\n",
      " 0.00763717 0.00763717 0.00763717 0.00763717 0.02027039 0.05502469\n",
      " 0.02027039 0.00763717 0.02027039]\n",
      "-------------------------------------\n",
      "-------------第4个决策树-------------\n",
      "-------------------------------------\n",
      "决策树桩：v = 3.725\n",
      "权重：alpha = 0.12276675782714255\n",
      "权值向量：D=[0.00680583 0.00680583 0.02309111 0.00680583 0.02309111 0.00869991\n",
      " 0.00869991 0.04903502 0.00869991 0.02309111 0.04903502 0.00869991\n",
      " 0.00869991 0.04903502 0.02309111 0.00680583 0.02309111 0.10678906\n",
      " 0.00869991 0.00680583 0.13014761 0.00869991 0.00869991 0.00869991\n",
      " 0.00869991 0.04903502 0.02309111 0.04903502 0.00869991 0.00869991\n",
      " 0.02309111 0.00680583 0.01060289 0.02309111 0.01060289 0.00680583\n",
      " 0.00869991 0.00869991 0.00869991 0.00680583 0.02309111 0.04903502\n",
      " 0.02309111 0.00869991 0.02309111]\n",
      "-------------------------------------\n",
      "-------------第5个决策树-------------\n",
      "-------------------------------------\n",
      "决策树桩：v = 2.3\n",
      "权重：alpha = 0.10928598537996881\n",
      "权值向量：D=[0.00613773 0.00613773 0.02082436 0.00613773 0.02082436 0.00784588\n",
      " 0.00784588 0.05502473 0.00784588 0.02082436 0.05502473 0.00784588\n",
      " 0.00784588 0.05502473 0.02082436 0.00613773 0.02082436 0.09630609\n",
      " 0.00784588 0.00613773 0.14604537 0.00784588 0.00784588 0.00784588\n",
      " 0.00784588 0.05502473 0.02082436 0.05502473 0.00784588 0.00784588\n",
      " 0.02082436 0.00613773 0.01189805 0.02082436 0.01189805 0.00613773\n",
      " 0.00784588 0.00784588 0.00784588 0.00613773 0.02082436 0.05502473\n",
      " 0.02082436 0.00784588 0.02082436]\n",
      "测试正确率为：1.0\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.core import debugger\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "debug = debugger.Pdb().set_trace\n",
    "\n",
    "#%matplotlib inline\n",
    "# 鸢尾花(iris)数据集\n",
    "# 数据集内包含 3 类共 150 条记录，每类各 50 个数据，\n",
    "# 每条记录都有 4 项特征：花萼长度、花萼宽度、花瓣长度、花瓣宽度，\n",
    "# 可以通过这4个特征预测鸢尾花卉属于（iris-setosa, iris-versicolour, iris-virginica）中的哪一品种。\n",
    "# 这里取60条记录，一项特征，两个类别。\n",
    "def create_data():\n",
    "    iris = load_iris()\n",
    "    df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "    df['label'] = iris.target\n",
    "    df.columns = ['sepal length', 'sepal width', 'petal length', 'petal width', 'label']\n",
    "    data = np.array(df.iloc[0:60, [0, 1, -1]])\n",
    "    for i in range(len(data)):\n",
    "        if data[i,-1] == 0:\n",
    "            data[i,-1] = -1\n",
    "    # print(data)\n",
    "    return data[:,1:2], data[:,-1]\n",
    "\n",
    "X, y = create_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=7)\n",
    "\n",
    "class BoostingTree(object):\n",
    "\n",
    "    def __init__(self,X_train, y_train, max_iteration = 5000, min_error = 0.001):\n",
    "        #数据个数\n",
    "        self.N = int(len(y_train))\n",
    "        #初始化权重矩阵，平均分配\n",
    "        self.D = np.array(self.N * [float(1/self.N)])\n",
    "\n",
    "        #当迭代次数大于最大迭代次数\n",
    "        #或者分类误差小于最小可接受误差时\n",
    "        #结束学习过程\n",
    "        #最大迭代次数\n",
    "        self.max_iteration = max_iteration\n",
    "        #最小可接受误差\n",
    "        self.min_error = min_error\n",
    "\n",
    "        #alpha矩阵，每个弱分类器的系数\n",
    "        #最多可有max_iteration个alpha_i\n",
    "        self.alpha = np.zeros(self.max_iteration)\n",
    "\n",
    "        #决策树桩矩阵\n",
    "        #见cacl_v_list(X_train,v_number)函数初始化\n",
    "        self.v_list = None\n",
    "        #决策值的索引列表\n",
    "        self.index_list = self.max_iteration*[0]\n",
    "\n",
    "    def print_init(self):\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"-----------初始化----------------\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(\"数据量:{}\".format(len(y_train)+len(y_test)))\n",
    "        print(\"训练数据量:{}\".format(len(y_train)))\n",
    "        print(\"测试数据量:{}\".format(len(y_test)))\n",
    "        print(\"D={}\".format(self.D))\n",
    "\n",
    "    def print_each_train(self,iteration,min_error_index,):\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"-------------第{}个决策树-------------\".format(iteration+1))\n",
    "        print(\"-------------------------------------\")\n",
    "        print(\"决策树桩：v = {}\".format(self.v_list[min_error_index]))\n",
    "        print(\"权重：alpha = {}\".format(self.alpha[iteration]))\n",
    "        print(\"权值向量：D={}\".format(self.D))\n",
    "    \n",
    "    #决策树函数\n",
    "    #算法中的Gm(x)\n",
    "    def G_x(self, X_data, v):\n",
    "        if X_data < v:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return -1.0\n",
    "        \n",
    "    #计算决策树桩的列表\n",
    "    #统计特征的最大最小值，通过numpy数据的linspace均分\n",
    "    #均分数人为确定\n",
    "    def cacl_v_list(self,X_train,v_number=100):\n",
    "        #self.v_list = X_train\n",
    "        x_min = np.min(X_train)\n",
    "        x_max = np.max(X_train)\n",
    "        self.v_list = np.linspace(x_min, x_max,v_number)\n",
    "    \n",
    "    #更新权重矩阵D\n",
    "    def update_D(self,alpha,X_train,y_train,index):\n",
    "        Zm = 0.0\n",
    "        for i in range(self.N):\n",
    "            Zm += self.D[i]*np.exp(-alpha*y_train[i]*self.G_x(X_train[i],self.v_list[index]))\n",
    "        for i in range(self.N):\n",
    "            self.D[i] = self.D[i]*np.exp(-alpha*y_train[i]*self.G_x(X_train[i],self.v_list[index]))/Zm\n",
    "    \n",
    "    #计算最小误差\n",
    "    #对v_list中的值进行迭代\n",
    "    #返回最小误差，与最小误差对应v的索引值\n",
    "    def CalcMinErrorAndInex(self,X_train,y_train):\n",
    "\n",
    "        min_error = 1000\n",
    "        min_error_index = 0\n",
    "\n",
    "        # 对v_list的每个v值进行迭代\n",
    "        # 求出最小的error与返回使error最小的v值在v_list中的index\n",
    "        for i in range(len(self.v_list)):\n",
    "            current_error = 0.00001\n",
    "            for j in range(self.N):\n",
    "                G_i_x = self.G_x(X_train[j], self.v_list[i])\n",
    "\n",
    "                if G_i_x != y_train[j]:\n",
    "                    current_error += self.D[j]\n",
    "\n",
    "            if current_error < min_error:\n",
    "                min_error = current_error\n",
    "                min_error_index = i\n",
    "\n",
    "        return min_error,min_error_index\n",
    "    \n",
    "    def train(self,X_train,y_train):\n",
    "        self.print_init()\n",
    "        #iteration的大小，决定分类树桩的个数\n",
    "        for iteration in range(self.max_iteration):\n",
    "            min_error, min_error_index = self.CalcMinErrorAndInex(X_train, y_train)\n",
    "\n",
    "            #如果当前误差小于可接受最小误差，结束学习\n",
    "            if (min_error <= self.min_error):\n",
    "                break\n",
    "\n",
    "            #记录本次迭代分类最小误差对应的v的索引\n",
    "            self.index_list[iteration]= min_error_index\n",
    "            self.alpha[iteration] = 0.5*np.log((1-min_error)/min_error)\n",
    "            self.update_D(self.alpha[iteration],X_train,y_train,self.index_list[iteration])\n",
    "            self.print_each_train(iteration,min_error_index)\n",
    "\n",
    "    def predict(self,X_test,y_test):\n",
    "\n",
    "        fx = 0\n",
    "        right_num = 0\n",
    "        for i in range(len(X_test)):\n",
    "            for j in range(len(self.alpha)):\n",
    "                fx += self.alpha[j]*self.G_x(X_test[i],self.v_list[self.index_list[j]])\n",
    "\n",
    "            if np.sign(fx) == y_test[i]:\n",
    "                right_num+=1\n",
    "            fx = 0\n",
    "        return right_num/len(y_test)\n",
    "\n",
    "\n",
    "boostingtree = BoostingTree(X_train,y_train,max_iteration = 5, min_error = 0.001)\n",
    "boostingtree.cacl_v_list(X_train,v_number=5)\n",
    "boostingtree.train(X_train,y_train)\n",
    "accuracy = boostingtree.predict(X_test,y_test)\n",
    "print(\"测试正确率为：{}\".format(accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 算法对比\n",
    "比较支持向量机,AdaBoost, 逻辑斯蒂回归模型的学习策略与算法\n",
    "\n",
    "| 模型 | 模型特点 | 模型类别 | 学习策略 | 学习算法 |\n",
    "| :------: | :------ | :------ |:------ |:------ |\n",
    "| 逻辑斯蒂回归 | 特征条件下类别的条件概率分布<br>对数线性模型 | 判别模型 |极大似然法|IIS,SGD,拟牛顿法\n",
    "| 支持向量机 | 分离超平面，核技巧 | 判别模型 |间隔最大化|SMO算法\n",
    "| 提升方法 | 弱分类器的线性组合 | 判别模型 |极小化加法模型的指数损失|前向分布算法"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Logistic Regression\n",
    "#### 2.1.1 模型\n",
    "逻辑斯蒂回归模型是满足以下概率分布的模型：<br>\n",
    "$$P(Y=k|x)=\\frac{exp(w_k\\cdot x)}{1+\\displaystyle \\sum^{K-1}_{k=1}{exp(w_k\\cdot x)}}$$<br>\n",
    "其中，$x\\in R^n$为输入,Y={1,2,...,K}为输出。<br>\n",
    "通过以上定义式，可以将线性函数$w \\cdot x$转化为概率。<br>\n",
    "当线性函数的值接近正无穷时，概率值就越接近1.<br>\n",
    "当线性函数的值接近负无穷时，概率值就越接近0.<br>\n",
    "哪个k对应的条件概率最大，该样本就被分到该类。\n",
    "\n",
    "#### 2.1.2 学习策略\n",
    "逻辑斯蒂回归模型学习的时候，对于给定的训练数据集:<br>\n",
    "$T = {(x_1,y_1),(x_2,y_2),...,(x_N,y_N)}$ <br>\n",
    "通过极大似然法估计参数模型，从而可以得到逻辑斯蒂回归模型\n",
    "\n",
    "#### 2.1.3 适用问题\n",
    "多分类\n",
    "\n",
    "### 2.2 SVM\n",
    "#### 2.2.1 线性可分支持向量机<br>\n",
    "##### 1 模型\n",
    "分离超平面：$w^{*}\\cdot x +b^{*} = 0$<br>\n",
    "分类决策函数：$f(x) = sign(w^{*}\\cdot x+b^{*})$<br>\n",
    "\n",
    "##### 2 优化问题（硬间隔最大化）：<br>\n",
    "$$\\displaystyle \\min_{w,b}{\\frac{1}{2}\\|w\\|^2}$$<br>\n",
    "$$s.t.\\: y_i(w \\cdot x_i +b)-1 \\geq 0, i=1,2,...,N$$<br>\n",
    "也可以通过对偶学习算法，构造拉格朗日函数，进行优化。\n",
    "\n",
    "#### 2.2.2 线性支持向量机<br>\n",
    "##### 1 模型\n",
    "分离超平面：$w^{*}\\cdot x +b^{*} = 0$<br>\n",
    "分类决策函数：$f(x) = sign(w^{*}\\cdot x+b^{*})$<br>\n",
    "\n",
    "##### 2 优化问题（软间隔最大化）：<br>\n",
    "引入松弛变量，容许误差，即容许噪声点。\n",
    "$$\\displaystyle \\min_{w,b}{\\frac{1}{2}\\|w\\|^2+C \\sum_{i=1}^{N}{\\xi}}$$<br>\n",
    "$$s.t.\\: y_i(w \\cdot x_i +b) \\geq 1-\\xi_{i}, i=1,2,...,N$$<br>\n",
    "$$\\: \\: \\: \\: \\: \\xi_{i}\\geq 0 ,i=1,2,...,N$$\n",
    "\n",
    "#### 2.2.3 非线性支持向量机<br>\n",
    "##### 1 模型\n",
    "分类决策函数:$$f(x) = sign(\\displaystyle \\sum_{i=1}^{N}{{\\alpha}^{*}_{i}y_iK(x,x_i)+b^{*}})$$<br>\n",
    "其中，K(x,z)为正定核函数<br>\n",
    "\n",
    "##### 2 优化问题\n",
    "$$\\displaystyle \\min_{\\alpha}{\\frac{1}{2}\\sum_{i=1}^{N}\\sum_{j=1}^{N}{\\alpha}_i{\\alpha}_jy_iy_jK(x_i,x_j)-\\sum_{i=1}^{N}{\\alpha_i}}$$<br>\n",
    "$$s.t.$$<br>\n",
    "$$\\displaystyle \\sum_{i=1}^{N}{{\\alpha}_iy_i=0}$$<br>\n",
    "$$0 \\leq {\\alpha}_i \\leq{C},i=1,2,...,N$$\n",
    "\n",
    "#### 2.2.4 适用问题\n",
    "二分类问题,经过多个SVM组合后也可适用于多分类问题。\n",
    "\n",
    "### 2.3 AdaBoost\n",
    "#### 2.3.1 模型\n",
    "每次学习一个弱分类器$G_m(x)$，最终将对此训练的弱分类器加和在一起，得到最终的强分类器$G(x)$。<br>\n",
    "基本分类器的线性组合：$$\\displaystyle f(x) = \\sum_{m=1}^{M}{{\\alpha}_mG_{m}(x)}$$<br>\n",
    "最终分类器：$\\displaystyle G(x) = sign(f(x)) = sign(\\sum_{m=1}^{M}{{\\alpha}_mG_{m}(x)})$\n",
    "#### 2.3.2 优化策略\n",
    "前向分布算法，每步最优。\n",
    "\n",
    "#### 2.3.3 适用问题\n",
    "二分类模型"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python361064bitcookconda2d3f5abb2ed14be8a6562d66d7e8fe13",
   "language": "python",
   "display_name": "Python 3.6.10 64-bit ('cook': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}